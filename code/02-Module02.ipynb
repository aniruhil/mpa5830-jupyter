{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802a3004-8917-45b3-8472-a3b4e99695a0",
   "metadata": {},
   "source": [
    "# MPA 5830 - Module 02 (Fall 2021)\n",
    "In this module our goal is to understand how some very powerful packages work their magic. In particular, although they are of somewhat recent vintage, `{dplyr}`, `{tidyr}` and `{lubridate}` have quickly gained a fan following because they allow you to clean and organize your data and then calculate quantities of interest with surprising ease. In this module we will start to see some of these packages' core functionalities and wrap up this particular leg of our learning journey in __`Module 03`__. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebb546",
   "metadata": {},
   "source": [
    "## `{dplyr}` \n",
    "There is a common quote that is tossed about a good bit in the hallways of data science, that, and I am paraphrasing here, a data scientists spends about 80% of their time gathering, cleaning, and organizing their data and spends only about 20% of their time on the analysis per se. This may or may not be true, especially in the initial stages of a new project but yes, we do spend an awfully large amount of our time getting the data ready for visualizations and other analyses. You do this work long enough and you come to realize that anything you could do to speed up the cleaning phase would be time and money saved. And yet, data cleaning skills are in short supply. On the plus side of the ledger, packages like `{dplyr}` and `{data.table}` have simplified what were once nightmarish tasks. \n",
    "\n",
    "`Nightmare` is not a word to be tossed around lightly and so let us setup a seemingly large data-set with 100+ columns, and tons of information hidden in it. Once we setup this scenario, spell out a few questions we would like to answer, we might better appreciate how `{dplyr}` comes to our aid. In particular, we might come to understand how `{dplyr}` uses seven core verbs:  \n",
    "\n",
    "| **What you want to do ...** | **`{dplyr}` function** |\n",
    "| :-----  | :----- |\n",
    "| you need to select columns to work with? | `select()`|\n",
    "| you need to use a subset of the data based on some criterion? | `filter()`|\n",
    "| you need to arrange the data in ascending/descending order of variable(s)? | `arrange()`|\n",
    "| you want the results of your calculations to be a standalone data frame? | `summarise()`|\n",
    "| you want to add your calculated value(s) to the existing data frame? | `mutate()`|\n",
    "| you want to add your calculated value(s) to the existing data frame but also  drop all  variables/columns not  being used in the calculation? | `transmute()`|\n",
    "| you need to calculate averages, frequencies, etc by groups? | `group_by()`|\n",
    "\n",
    "Here is the dataset -- all flights originating and departing from Columbus (Ohio) January through September of 2017. \n",
    "\n",
    "Let  us load the data, and the `{tidyverse}` and `{here}` packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f46fac-acd4-4114-ab41-86aec82eede9",
   "metadata": {
    "name": "cmhflights"
   },
   "outputs": [],
   "source": [
    "library(tidyverse) \n",
    "\n",
    "load(\"data/cmhflights_01092017.RData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e0c97-f6e5-4c21-9ed9-ef707cf3a6b9",
   "metadata": {
    "name": "cmhflights"
   },
   "outputs": [],
   "source": [
    "names(cmhflights) # will show you the column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c1e09",
   "metadata": {},
   "source": [
    "The output is rather onerous with 110 columns displayed, the last being `X110`, an empty column that can be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f5638",
   "metadata": {
    "name": "dropx110"
   },
   "outputs": [],
   "source": [
    "cmhflights$X110 <- NULL #This command will delete the column named X110 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5562779e-3163-4b37-b75c-ccd531ca290a",
   "metadata": {},
   "source": [
    "Did it work? Let us check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab27d7-daa5-4fbc-83b5-b53cc8ee5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "names(cmhflights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8378e0",
   "metadata": {},
   "source": [
    "Now we are down to 109 columns and ready to get to work. \n",
    "\n",
    "## select() \n",
    "As in the present case of 100+ columns, often our data frame will have more columns than we plan to work with. In such instances it is a good idea to drop all unwanted columns; this will make the data-set more manageable and tax our computing resources less. For example, say I only want the first five columns (which happen to be `Year`, `Quarter`, `Month`, `DayofMonth`, and `DayOfWeek`). I could use `select` to create a data frame with only these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca453e7",
   "metadata": {
    "name": "select01a"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  select(Year:DayOfWeek) -> my.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c8abb-262b-42b9-9393-5ee273f3b309",
   "metadata": {},
   "source": [
    "The `:` is the bridge between __consecutive columns__ starting with `Year` and stopping with `DayofWeek`.\n",
    "\n",
    "In addition, note that the subset of columns selected and all rows of data are being written to a new data frame called `my.df`\n",
    "\n",
    "Quick check to see if we have only the columns we wanted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6c836-9acc-4dc1-9267-660c1a3bbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "names(my.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f7192",
   "metadata": {},
   "source": [
    "Wonderful! It worked.\n",
    "\n",
    "Now, the same result could have been obtained by taking the longer route of __listing each column by name__, as in the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b6756",
   "metadata": {
    "name": "select01b"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  select(Year, Quarter, Month, DayofMonth, DayOfWeek) -> my.df \n",
    "\n",
    "names(my.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce667f3f",
   "metadata": {},
   "source": [
    "What if the columns were not sequentially located? In that case we would need to list each column we want. Say I want `Year`, `FlightDate`, `UniqueCarrier`, and `TailNum`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f62aa7",
   "metadata": {
    "name": "select02"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  select(Year, FlightDate:UniqueCarrier, TailNum) -> my.df \n",
    "\n",
    "names(my.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b6e0e",
   "metadata": {},
   "source": [
    "Could we have used __column numbers instead of column names__? Absolutely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68812eb6",
   "metadata": {
    "name": "select03a"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  select(c(1, 3, 5, 7)) -> my.df \n",
    "\n",
    "names(my.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4a587-a794-4c60-abdd-ac706bc243ab",
   "metadata": {},
   "source": [
    "You can also use __consecutive column numbers__, for examples, columns 1 through 5 as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e737532",
   "metadata": {
    "name": "select03b"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  select(c(1:5)) -> my.df \n",
    "\n",
    "names(my.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6cc91-9f3a-4921-a225-101fe4b885b4",
   "metadata": {},
   "source": [
    "You can also use column numbers to select a mix of columns, some that may be sequential and others that may be not sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828c779",
   "metadata": {
    "name": "select03c"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  select(c(1, 6:9, 12)) -> my.df\n",
    "\n",
    "names(my.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624aaa4",
   "metadata": {},
   "source": [
    "### select() in other ways\n",
    "We can also select columns in other ways, by specifying that  the column name `contain` some element. The code below shows you how this is done if I am looking for column names with the phrase \"Carrier\", then with \"De\", and then with \"Num\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b516f7d-67b2-4548-b08e-8e775609e5c4",
   "metadata": {
    "name": "select04"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  select(contains(\"Carrier\")) -> my.df\n",
    "\n",
    "names(my.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f461bc-f8c5-437a-b03f-64e186811d96",
   "metadata": {},
   "source": [
    "You can also specify that the columns to be selected __start with__ some alphanumeric string, for example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef7b7f-360e-4cf4-b256-cedc3666f367",
   "metadata": {
    "name": "select04"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  select(starts_with(\"De\")) -> my.df\n",
    "\n",
    "names(my.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63741e81-556a-4ab4-b076-130519121c7a",
   "metadata": {},
   "source": [
    "The other option would be to choose columns that __end with__ a particular alphanumeruc string, for example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300da4d-39c2-4b29-8e59-234ea4cf2e31",
   "metadata": {
    "name": "select04"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  select(ends_with(\"Num\")) -> my.df\n",
    "\n",
    "names(my.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1497a-202d-4363-a469-486ba332e990",
   "metadata": {},
   "source": [
    "There are two other options -- `matches()` and `num_range()`. Let us look at each in turn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c6744-1a81-406f-ab3a-74fd0f6179a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "    select(matches(\"el\")) -> my.df\n",
    "\n",
    "names(my.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f86b6d-41a3-46c0-ad18-bccb48c155ae",
   "metadata": {},
   "source": [
    "Here is another dataset where some of the column names contain a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2141d6-1527-4dbc-aeda-7e9de032bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(billboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f183e46a-97aa-4b62-9e3f-efbaf4a6aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard %>%\n",
    "    select(num_range(\"wk\", 1:5)) -> my.df\n",
    "\n",
    "names(my.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77935ac-9f97-49a0-ba3f-dccb728caeb6",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd65a5",
   "metadata": {},
   "source": [
    "## filter()\n",
    "Do you really want all the rows in the data-set or do you want to see only very specific rows that meet some criteria? Say we only want to look at certain months, or only flights on Saturdays and Sundays, or flights in a given month. For example, say we only want flights in January, i.e., `Month == 1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab1f065",
   "metadata": {
    "name": "filter01"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  filter(Month == 1) -> my.df\n",
    "\n",
    "table(my.df$Month) # Show me a frequency table for Month in my.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f32533",
   "metadata": {},
   "source": [
    "What about only American Airline flights in January? Note that the UniqueCarrier code for this airline is AA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aeefa1",
   "metadata": {
    "name": "filter02"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  filter(Month == 1, UniqueCarrier == \"AA\") -> my.df \n",
    "\n",
    "table(my.df$Month, my.df$UniqueCarrier) # a simple frequency table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587da241",
   "metadata": {},
   "source": [
    "Note that `,` inside `filter` means `&`\n",
    "\n",
    "What about United Airlines flights in January to CMH (the airport code for Columbus, OH), regardless of where the flight originated? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431c51c",
   "metadata": {
    "name": "filter03"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  filter(Month == 1, UniqueCarrier == \"UA\", Dest == \"CMH\") -> my.df\n",
    "\n",
    "head(my.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12b9d7",
   "metadata": {},
   "source": [
    "What if I wanted a more complicated filter, say, flights in January or February, either to CMH or to ORD (the airport code for O'Hare in Chicago)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a573b3-fb65-4c11-bd30-483989a1d548",
   "metadata": {
    "name": "filter04"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  filter(\n",
    "    Month %in% c(1, 2), UniqueCarrier == \"UA\", Dest %in% c(\"CMH\", \"ORD\")\n",
    "    ) -> my.df \n",
    "\n",
    "table(my.df$Month) # frequency table of Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00ae03-2014-4006-afae-ba213da2eeda",
   "metadata": {
    "name": "filter04"
   },
   "outputs": [],
   "source": [
    "table(my.df$UniqueCarrier) # frequency table of UniqueCarrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd7ee0-68bb-4315-a1ca-099ffc72f89f",
   "metadata": {
    "name": "filter04"
   },
   "outputs": [],
   "source": [
    "table(my.df$Dest) # frequency table of Dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d35316-cd67-4c77-8bff-b5b1373c5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  filter(\n",
    "    !Month %in% c(1, 2)\n",
    "    ) -> my.df \n",
    "\n",
    "table(my.df$Month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53fa4cc-728c-46a9-8239-f63943bb3be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "filter(\n",
    "    Month != 1, Month != 2\n",
    "    ) -> my.df\n",
    "\n",
    "table(my.df$Month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9a808",
   "metadata": {},
   "source": [
    "Beautiful, just beautiful. \n",
    "\n",
    "At this point it may not be readily apparent to you but using `%in% c(...)` makes applying complex filters easier than if you go some other route. \n",
    "\n",
    "Before we move on, note the operators  that work with `filter()` \n",
    "\n",
    "| **Operator** | **Meaning**  | **Operator** | **Meaning** |\n",
    "| :---- | :---- | :---- |  :---- |\n",
    "| $<$    | less than | $>$    | greater than |\n",
    "| $==$   | equal to  | $\\leq$ | less than or equal to |\n",
    "| $\\geq$ | greater than or equal to | `!=` | not equal to |\n",
    "| `%in%`   | is a member of |  `is.na` | is NA |\n",
    "| `!is.na` | is not NA  | `&,!,etc` | Boolean operators |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f49ea-3a6d-4b24-8722-0dac2a1533a2",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbd162",
   "metadata": {},
   "source": [
    "## arrange()\n",
    "\n",
    "Now let us say I wanted to arrange the resulting data frame in `ascending order` of departure delays. How might I do that? Via `arrange()` \n",
    "\n",
    "Before we see this new command in action I am going to whittle the data-frame to only a few columns, and only flights to CMH or ORD. That will make it easier to see the result of executed commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f282aa",
   "metadata": {
    "name": "arrange01"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "    select('Year', 'Month', 'DayofMonth', 'FlightDate', 'Carrier', 'TailNum', 'FlightNum', \n",
    "           'Origin', 'OriginCityName', 'Dest', 'DestCityName', 'CRSDepTime', 'DepTime', \n",
    "           'DepDelay', 'DepDelayMinutes', 'CRSArrTime', 'ArrTime', 'ArrDelay', 'ArrDelayMinutes') %>%\n",
    "    filter(Dest %in% c(\"CMH\", \"ORD\")) -> my.df\n",
    "\n",
    "my.df %>% \n",
    "  arrange(DepDelayMinutes) -> my.df2\n",
    "\n",
    "my.df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc711f2",
   "metadata": {},
   "source": [
    "And now in `descending order` of delays by adding the minus symbol `-` to the  column name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed715d9",
   "metadata": {
    "name": "arrange02"
   },
   "outputs": [],
   "source": [
    "my.df %>% \n",
    "  arrange(-DepDelayMinutes) -> my.df2\n",
    "\n",
    "my.df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7d950",
   "metadata": {},
   "source": [
    "We could tweak this further, perhaps saying sort by departure delays to CMH, and then to ORD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38a0b3",
   "metadata": {
    "name": "arrange03"
   },
   "outputs": [],
   "source": [
    "my.df %>% \n",
    "  arrange(Dest, -DepDelayMinutes) -> my.df2\n",
    "\n",
    "my.df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d696c4",
   "metadata": {},
   "source": [
    "So far, we have seen each function in isolation. Now we streamline things a bit so that we only end up with the columns and rows we want to work with, arranged as we want the resulting data-set to be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb2b98",
   "metadata": {
    "name": "stringing01"
   },
   "outputs": [],
   "source": [
    "cmhflights %>% \n",
    "  select(Month, UniqueCarrier, Dest, DepDelayMinutes) %>% \n",
    "  filter(\n",
    "    Month %in% c(1, 2), UniqueCarrier == \"UA\", Dest %in% c(\"CMH\", \"ORD\")\n",
    "    ) %>% \n",
    "  arrange(Month, Dest, -DepDelayMinutes) -> my.df3\n",
    "\n",
    "my.df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1378b1",
   "metadata": {},
   "source": [
    "Here, the end result is a data frame arranged by Month, then within Month by Destination, and then finally by descending order of flight delays. This is the beauty of `dplyr`, allowing us to chain together various functions to get what we want. How is this helpful? Well, now you have a data frame that you can analyze. What do I want to calculate? Well, let us say we want to create a frequency table, something that we would have done with Excel via a Pivot Table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb870085-d8bf-4f03-8677-48f204a6699f",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a780220e-a8f5-499e-88c6-51863d508129",
   "metadata": {},
   "source": [
    "## summarise() or summarize()\n",
    "What if we need to calculate frequencies? For example, how many flights per month are there? What if we want the __mean__ DepDelay or __median__ ArrDelay? These can be easily calculated as shown below. \n",
    "\n",
    "Let us start with frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfbd1d",
   "metadata": {
    "name": "summarise01"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  count(Month) # Most flights were in July (n = 4295)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a62b6c",
   "metadata": {},
   "source": [
    "What about by days of the week AND by month? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920cd89",
   "metadata": {
    "name": "summarise02"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  count(Month, DayOfWeek) # Output is sorted by Month and then DayOfWeek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f12127",
   "metadata": {},
   "source": [
    "I want to know the average departure delay and the average arrival delay for all flights, with the averages calculated in two ways -- as the `mean` and as the `median`. Maybe I also want the variance and the standard deviation of both delays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad0d1f",
   "metadata": {
    "name": "summarise03"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  summarise(\n",
    "      mean_arr_delay = mean(ArrDelay, na.rm = TRUE),\n",
    "      mean_dep_delay = mean(DepDelay, na.rm = TRUE),\n",
    "      median_arr_delay = median(ArrDelay, na.rm = TRUE),\n",
    "      median_dep_delay = median(DepDelay, na.rm = TRUE),\n",
    "      variance_arr_delay = var(ArrDelay, na.rm = TRUE),\n",
    "      variance_dep_delay = var(DepDelay, na.rm = TRUE),  \n",
    "      sd_arr_delay = sd(ArrDelay, na.rm = TRUE),\n",
    "      sd_dep_delay = sd(DepDelay, na.rm = TRUE)\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa30957",
   "metadata": {},
   "source": [
    "Here, the `na.rm = TRUE` command is useful because R will not allow you to calculate any mean, median, variance, etc. if the clumn includes some rows with missing data. \n",
    "\n",
    "You can see this below, where I have a small data-set called `df` with 4 values of x, but one of the four is missing and recorded as `NA`. See what happens when I try to calculate the mean with/without `na.rm = TRUE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26caef-be20-4ba7-a8c1-426d3529fade",
   "metadata": {
    "name": "narm"
   },
   "outputs": [],
   "source": [
    "df = data.frame(x = c(2, 4, 9, NA))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf2582-87b6-4236-bee9-b700a5e67248",
   "metadata": {
    "name": "narm"
   },
   "outputs": [],
   "source": [
    "df %>%\n",
    "  summarise(mean.x = mean(x)) # You get no meaningful values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfabea3-775d-4e16-858a-b118bf08b2af",
   "metadata": {
    "name": "narm"
   },
   "outputs": [],
   "source": [
    "df %>%\n",
    "  summarise(mean.x = mean(x, na.rm = TRUE)) # Now you do get something meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad5613-ed3e-40fa-a9c5-8984c236051e",
   "metadata": {},
   "source": [
    "What if I want to count total DepDelay by airline? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019038f-b498-4d6c-8d7c-d51706beac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "    group_by(Carrier) %>%\n",
    "    summarize(sum(DepDelay, na.rm = TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c4c40-ae4a-4041-aa35-5cbf5cc90e50",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6e6d9",
   "metadata": {},
   "source": [
    "## group_by()\n",
    "These summaries are fine if you want to calculate aggregate quantities of interest (i.e., means, medians, frequencies, variances, etc. for all rows of data) but what if you wanted to calculate the __number of flights per month, by airline? Average delays by airline?__ \n",
    "\n",
    "Now things get interesting because `group_by()` will open up this new world for us! The first thing I will calculate is the number of flights by airline per month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe8ff5-8cf1-4d97-88b6-e1f5ad2b3d9e",
   "metadata": {
    "name": "groupby01"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  group_by(Month, Carrier) %>%\n",
    "  tally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc4886-2118-4eae-8d19-416e6050183a",
   "metadata": {},
   "source": [
    "You can get the same result if you use `summarize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce30d7-a5af-43c6-a9c7-be74da2cd12f",
   "metadata": {
    "name": "groupby01"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  group_by(Month, Carrier) %>%\n",
    "  summarize(\n",
    "    frequency = n()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e279da",
   "metadata": {},
   "source": [
    "Top recap, two ways to do this, first with `tally()` and the second with `summarize(frequency = n())` ... both yielding the same result. Remember `tally()` because it is shorter code. \n",
    "\n",
    "Now I want a table that gives us the number of flights per month per airline per destination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6fbeb",
   "metadata": {
    "name": "groupby02"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  group_by(Month, Carrier, Dest) %>%\n",
    "  tally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3640b7d5",
   "metadata": {},
   "source": [
    "We could keep enriching the grouping structure. For example, let us add the day of the week to the mix ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396568eb",
   "metadata": {
    "name": "groupby03"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  group_by(Month, Carrier, Dest, DayOfWeek) %>%\n",
    "  tally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d643f6f2",
   "metadata": {},
   "source": [
    "Now say I am really curious about mean departure delays for the preceding grouping structure. That is, what does mean departure delay look like for flights by day of the week, by month, by carrier, and by destination?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582175f2",
   "metadata": {
    "name": "groupby04"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  group_by(Month, Carrier, Dest, DayOfWeek) %>%\n",
    "  summarise(mean_dep_delay = mean(DepDelay, na.rm = TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f836cb",
   "metadata": {},
   "source": [
    "But this is a complicated summary table. What if all I really want to know is what airline has the __highest mean departure delays__, regardless of month or destination or day of the week? This could be done by using `arrange()` to display the result in descending order of _mean_dep_delay_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d36fbc",
   "metadata": {
    "name": "groupby05"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  group_by(Carrier) %>%\n",
    "  summarise(mean_dep_delay = mean(DepDelay, na.rm = TRUE)) %>%\n",
    "  arrange(-mean_dep_delay) # ordered in descending order of delays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342a971",
   "metadata": {},
   "source": [
    "EV is Express Jet; F9 is Frontier Airlines; WN is Southwest Airlines; OO is SkyWest Airlines; AA is American Airlines; DL is Delta Airlines; UA is United Airlines. So clearly United Airlines had the lowest average departure delays. Would this still be true if we repeated the calculation by Month? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2561e",
   "metadata": {
    "name": "groupby06"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  group_by(Carrier, Month) %>%\n",
    "  summarise(mean_dep_delay = mean(DepDelay, na.rm = TRUE)) %>%\n",
    "  arrange(mean_dep_delay) # ordered in descending order of delays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b6691c",
   "metadata": {},
   "source": [
    "All righty then! Looks like three of the lowest mean departure delays were for SkyWest. Do not let the negative numbers throw you for a loop; a negative value implies the flight left earlier than scheduled. \n",
    "\n",
    "So far so good. But now I am curious about what percent of flights operated by AA, DL, UA, and WN were delayed. How could I calculate this? \n",
    "\n",
    "(1) I need to use `filter()` to restrict the data-set to just these four airlines.  \n",
    "(2) Then I need to generate a new column that identifies whether a flight was delayed or not (`late`).  \n",
    "(3) Now we can calculate the total number of flights (`nflights`) and the total number of flights that were delayed (`nlate`).  \n",
    "(4) If I then calculate $\\left( \\dfrac{nlate}{nflights} \\right)\\times 100$ we will end up with the percent of flights that were delayed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e54ba",
   "metadata": {
    "name": "groupby07"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  select(c(Carrier, DepDelay)) %>%\n",
    "  filter(\n",
    "      Carrier %in% c(\"AA\", \"DL\", \"UA\", \"WN\")\n",
    "  ) %>%\n",
    "  mutate(\n",
    "      late = case_when(\n",
    "          DepDelay > 0 ~ \"Yes\",\n",
    "          DepDelay <= 0 ~ \"No\"\n",
    "      )\n",
    "  ) %>% \n",
    "  group_by(Carrier) %>%\n",
    "  mutate(\n",
    "      nflights = n()\n",
    "  ) %>%\n",
    "  group_by(Carrier, late) %>%\n",
    "  mutate(\n",
    "    nlate = n(),\n",
    "    pct_late = (nlate / nflights) * 100\n",
    "  ) -> df1\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9144892",
   "metadata": {},
   "source": [
    "There is a whole lot going on here so let us break it down. \n",
    "\n",
    "`filter(Carrier %in% c(\"AA\", \"DL\", \"UA\", \"WN\"))` is keeping specified airlines' data while dropping the rest\n",
    "\n",
    "`mutate(late = case_when(...)` is creating a new column called `late` and storing a value of \"Yes\" if `DepDelay > 0` (i.e., the flight was delayed by 1 or more minutes) and \"No\" if `DepDelay <= 0` (i.e., if the flight departed on time or earlier than the scheduled departure time) \n",
    "\n",
    "`group_by(Carrier)` is grouping by Carrier and then counting with `mutate(nflights = n())` how many flights there were per Carrier and storing this sum in a new column called `nflights`\n",
    "\n",
    "`group_by(Carrier, late)` then regroups the data, this time by Carrier and if the flight was late or not\n",
    "\n",
    "\n",
    "`mutate(nlate = n(), pct_late = (nlate / nflights) * 100)` is then creating two new columns, `nlate` -- the number of flights per late values of \"Yes\" and \"No\", respectively, and then `pct_late` -- the percent of flights per carrier that were late.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2eb2a-cc2f-452a-956e-ad1b818d7003",
   "metadata": {},
   "source": [
    "Now, we only want the flights that were late so let us apply `select()` to keep just a few columns and then we use `filter()` to keep only rows corresponding to `late = \"Yes\"`. \n",
    "\n",
    "This will still leave us with duplicate rows but we can drop these duplicate rows via a new command, `distinct()`  -- which if left empty inside the parentheses `()` looks for all unique rows of data (each row ends up with a unique combination of all columns' values). If you want unique values of specific columns then those column names can be inserted inside the parentheses `()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccde5aa",
   "metadata": {
    "name": "groupby08"
   },
   "outputs": [],
   "source": [
    "df1 %>%\n",
    "  filter(late == \"Yes\") %>%\n",
    "  ungroup() %>%\n",
    "  select(Carrier, pct_late) %>%\n",
    "  distinct() %>%\n",
    "  arrange(pct_late)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5417b0",
   "metadata": {},
   "source": [
    "So! 24% of UA flights were late, the lowest in this group. \n",
    "\n",
    "What if we wanted to do this for all airlines, and we want the calculations to be done by Month as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce9e61",
   "metadata": {
    "name": "groupby09"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  select(c(Carrier, Month, DepDelay)) %>%\n",
    "#  filter(Carrier %in% c(\"AA\", \"DL\", \"UA\", \"WN\")) %>%\n",
    "  mutate(late = case_when(\n",
    "    DepDelay > 0 ~ \"Yes\",\n",
    "    DepDelay <= 0 ~ \"No\"\n",
    "      )\n",
    "    ) %>% \n",
    "  group_by(Carrier, Month) %>%\n",
    "  mutate(nflights = n()) %>%\n",
    "  group_by(Carrier, Month, late) %>%\n",
    "  mutate(\n",
    "    nlate = n(),\n",
    "    pct_late = (nlate / nflights) * 100) %>%\n",
    "  filter(late == \"Yes\") %>%\n",
    "  ungroup() %>%  \n",
    "  select(Carrier, Month, pct_late) %>%\n",
    "  distinct() %>%\n",
    "  arrange(pct_late)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad65ece",
   "metadata": {},
   "source": [
    "Before we move on, I want to point out something about `case_when()`. Specifically, we used it to create a new column called `late` from numeric values found in `DepDelay`. But what if we wanted to create a new column from a column that had categorical variables in it, like `Dest` or `Carrier`? Easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d6204",
   "metadata": {
    "name": "casewhen01"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "    mutate(\n",
    "      carrier_name = case_when(\n",
    "          Carrier == \"AA\" ~ \"American Airlines\",\n",
    "          Carrier == \"DL\" ~ \"Delta Airlines\",\n",
    "          Carrier == \"UA\" ~ \"United Airlines\",\n",
    "          Carrier == \"EV\" ~ \"Express Jet\",\n",
    "          Carrier == \"F9\" ~ \"Frontier Airlines\",\n",
    "          Carrier == \"WN\" ~ \"Southwest Airlines\",\n",
    "          Carrier == \"OO\" ~ \"SkyWest Airlines\"\n",
    "      )\n",
    "  ) %>%\n",
    "    select(Carrier, carrier_name) %>%\n",
    "    group_by(Carrier, carrier_name) %>%\n",
    "    tally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae3d38",
   "metadata": {},
   "source": [
    "Second, `case_when()` includes an option that cuts down on our work. In particular, say I want to create a new column and label its values as \"Weekend\" if the DayOfWeek is Saturday or Sunday and \"Weekday\" if DayOfWeek is any other day. In doing this, it would serve us well to remember that the week begins on Sunday so DayOfWeek == 1 is Sunday, not Monday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32079281",
   "metadata": {
    "name": "casewhen02"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  mutate(\n",
    "      weekend = case_when(\n",
    "          DayOfWeek %in% c(7, 1) ~ \"Yes\",\n",
    "          TRUE ~ \"No\"\n",
    "      )\n",
    "    ) %>%\n",
    "  select(DayOfWeek, weekend) %>%\n",
    "  distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e231f59",
   "metadata": {},
   "source": [
    "Notice how `TRUE` swept up all other values of `DayOfWeek` and coded them as \"No.\"  \n",
    "\n",
    "One final showcasing of `case_when()`. In **Module 01** we looked at the `hsb2` data and created some `factors` for columns such as female, ses, schtyp, and so on. Well, let us see how the same thing could be done with `case_when()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd4f1c-e26d-4a93-a411-a9860da3ca30",
   "metadata": {
    "lines_to_next_cell": 0,
    "name": "casewhen03"
   },
   "outputs": [],
   "source": [
    "read.table(\n",
    "  'https://stats.idre.ucla.edu/stat/data/hsb2.csv',\n",
    "  header = TRUE,\n",
    "  sep = \",\"\n",
    "  ) -> hsb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adac529-7c9a-4ec9-9b8b-cedd4f31daf8",
   "metadata": {
    "lines_to_next_cell": 0,
    "name": "casewhen03"
   },
   "outputs": [],
   "source": [
    "hsb2 %>%\n",
    "  mutate(\n",
    "    female.f = case_when(\n",
    "      female == 0 ~ \"Male\",\n",
    "      female == 1 ~ \"Female\"),\n",
    "    race.f = case_when(\n",
    "      race == 1 ~ \"Hispanic\",\n",
    "      race == 2 ~ \"Asian\",\n",
    "      race == 3 ~ \"African-American\",\n",
    "      TRUE ~ \"White\"),\n",
    "    ses.f = case_when(\n",
    "      ses == 1 ~ \"Low\",\n",
    "      ses == 2 ~ \"Medium\",\n",
    "      TRUE ~ \"High\"),\n",
    "    schtyp.f = case_when(\n",
    "      schtyp == 1 ~ \"Public\",\n",
    "      TRUE ~ \"Private\"),\n",
    "    prog.f = case_when(\n",
    "      prog == 1 ~ \"General\",\n",
    "      prog == 2 ~ \"Academic\",\n",
    "      TRUE ~ \"Vocational\")\n",
    "    ) -> hsb2\n",
    "\n",
    "hsb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2007a",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5c608",
   "metadata": {},
   "source": [
    "## Some other `dplyr()` commands\n",
    "We have seen `count()` in action but let us see it again, in a slightly different light. In particular, say I want to know how many unique destinations are there connected by air from Columbus. \n",
    "\n",
    "### count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1434052",
   "metadata": {
    "name": "count01"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  filter(Origin == \"CMH\") %>%\n",
    "  count(Dest, sort = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acd470",
   "metadata": {},
   "source": [
    "Note: There is no need for `group_by()` here. And `sort = TRUE` arranges the result in descending order of the frequency (`n`). Here is another code example, this time adding Carrier to the mix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582a2b3",
   "metadata": {
    "name": "count02"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  filter(Origin == \"CMH\") %>%\n",
    "  count(Carrier, Dest, sort = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2379438",
   "metadata": {},
   "source": [
    "How does this help us? Well, now we know that if we were flying to Atlanta, Delta would have the most flights, but if we were flying to the Chicago area then Southwest should be our pick.  \n",
    "\n",
    "### n_distinct() \n",
    "Another useful command is `n_distinct()`, useful in the sense of allowing us to calculate the the number of distinct values of any column. For example, say I want to know how many unique aircraft (not airlines) are there in this data-set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a5317",
   "metadata": {
    "lines_to_next_cell": 2,
    "name": "ndistinct01"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  summarise(n_distinct(TailNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04379bf4",
   "metadata": {},
   "source": [
    "### top_n()\n",
    "If you want to see the top 'n' number of observations, for example the 4 airlines with the most aircraft, you can lean on `top_n()`, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f4ed3",
   "metadata": {
    "name": "topn01"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  group_by(Carrier) %>%\n",
    "  summarise(num.flights = n_distinct(TailNum)) %>%\n",
    "  arrange(-num.flights) %>% \n",
    "  top_n(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8eccef",
   "metadata": {},
   "source": [
    "I am also curious about which aircraft has flown the most, and then maybe 9 other aircraft that follow in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63071a",
   "metadata": {
    "name": "topn02"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  filter(!is.na(TailNum)) %>% # Removing some missing cases \n",
    "  group_by(TailNum) %>%\n",
    "  tally() %>% \n",
    "  arrange(-n) %>%\n",
    "  top_n(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f201f3da",
   "metadata": {},
   "source": [
    "### join()\n",
    "You will, from time to time, need to merge multiple data-sets together. For example, say I have the following data-sets I have created for demonstration purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f3d5c5-8bf4-4656-9216-bc7be7cd6c97",
   "metadata": {
    "name": "join0"
   },
   "outputs": [],
   "source": [
    "tibble(\n",
    "  Name = c(\"Tim\", \"Tammy\", \"Bubbles\", \"Panda\"),\n",
    "  Score = c(5, 8, 9, 10)\n",
    "    ) -> df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8a007-d759-48f8-b684-8bbf52221bf6",
   "metadata": {
    "name": "join0"
   },
   "outputs": [],
   "source": [
    "tibble(\n",
    "  Name = c(\"Tim\", \"Tammy\", \"Bubbles\"),\n",
    "  Age = c(25, 78, 19)\n",
    "    ) -> df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793c34a-1916-4e65-8c48-1c1cc53cbbd3",
   "metadata": {
    "name": "join0"
   },
   "outputs": [],
   "source": [
    "tibble(\n",
    "  Name = c(\"Tim\", \"Tammy\", \"Panda\"),\n",
    "  Education = c(\"BA\", \"PhD\", \"JD\")\n",
    "    ) -> df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9eff4-3cc5-4af8-aed6-3dafd8cf51e8",
   "metadata": {
    "name": "join0"
   },
   "outputs": [],
   "source": [
    "df1; df2; df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39a0dc",
   "metadata": {},
   "source": [
    "Notice that Panda is absent from `df2` and Bubbles is absent from `df3`. So if we wanted to build ONE data-set with all data for Tim, Tammy, Bubbles, and Panda, some of the information would be missing for some of these folks. But how could we construct ONE data-set? Via one of a few `join()` commands. \n",
    "\n",
    "#### full_join() \n",
    "Let us start with a simple full_join, where we link up every individual in df1 or df2 or df3 **regardless of whether they are seen in both data-sets**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a22fc",
   "metadata": {
    "name": "join02"
   },
   "outputs": [],
   "source": [
    "df1 %>%\n",
    "  full_join(df2, by = \"Name\") %>%\n",
    "  full_join(df3, by = \"Name\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c445c",
   "metadata": {},
   "source": [
    "Pay attention to two things: (i) Name connects the records in each data-set, and so it must be spelled exactly the same for a link to be made, and (ii) the `full_join()` links up all individuals regardless of whether they are missing any information in any of the data-sets. This is usually how most folks will link up multiple files unless they only want records found in a master file. For example, say I want to link up df2 and df3 but only such that the final result will include all records found in BOTH df2 and df3, with df2 serving as the master data-set. Eh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94c28a",
   "metadata": {
    "name": "join03"
   },
   "outputs": [],
   "source": [
    "df2 %>%\n",
    "  left_join(df3, by = \"Name\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd91b7",
   "metadata": {},
   "source": [
    "Notice that Panda is dropped because it is not found in df2. \n",
    "\n",
    "Maybe you want df3 to be the master file, in which case you would see a different result (with Bubbles not seen in the result since Bubbles is found in df2 but not in df3): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bab778",
   "metadata": {
    "name": "join04"
   },
   "outputs": [],
   "source": [
    "df3 %>%\n",
    "  left_join(df2, by = \"Name\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f17222",
   "metadata": {},
   "source": [
    "Rarely, but definitely not \"never,\" you may want to see the records that are not found in both. Here, anti_join() comes in handy, thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458205a-8fc4-4d86-a61e-6e16002eeb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 %>%\n",
    "  anti_join(df3, by = \"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb6fa8-eeaa-4511-9d7e-3172349d2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 %>%\n",
    "  anti_join(df2, by = \"Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f1a72-ac94-4081-9fae-2b4dccff2743",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61640ad",
   "metadata": {},
   "source": [
    "## Two other useful commands \n",
    "### {santoku}\n",
    "Every now and then you may want to or need to create a grouped version of some numeric variable. For example, we have DepDelay for all flights but want to group this into `quartiles`. How can we do that? In many ways but the easiest might be to use a specific library -- `{santoku}`. Say, for example, I want to create 4 groups of `dep_delay`, and I want these such that we are grouping `DepDelay` into the bottom 25%, next 25%, the next 25%, and finally the highest 25%. Wait, these are the `quartiles`! Fair enough, but how can I do this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52acdbc5",
   "metadata": {
    "name": "santoku01"
   },
   "outputs": [],
   "source": [
    "library(santoku)\n",
    "\n",
    "cmhflights %>%\n",
    "  mutate(\n",
    "    depdelay_groups = chop_equally(DepDelay, groups = 4)\n",
    "      ) %>%\n",
    "  group_by(depdelay_groups) %>%\n",
    "  tally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153a69a",
   "metadata": {},
   "source": [
    "What if we wanted to slice up DepDelay in specific intervals, first at 0, then at 15, then at 30, and then at 45? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d8ce35",
   "metadata": {
    "lines_to_next_cell": 2,
    "name": "santoku02"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  mutate(\n",
    "    depdelay_groups = chop(DepDelay, breaks = c(0, 15, 30, 45))\n",
    "      ) %>%\n",
    "  group_by(depdelay_groups) %>%\n",
    "  tally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516f9c6",
   "metadata": {},
   "source": [
    "We could also create quintiles (5 groups) or deciles (10 groups) as shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2ad59",
   "metadata": {
    "lines_to_next_cell": 2,
    "name": "santoku03"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  filter(!is.na(DepDelay)) %>%\n",
    "  mutate(\n",
    "    depdelay_groups = chop_quantiles(\n",
    "      DepDelay, c(0.2, 0.4, 0.6, 0.8)\n",
    "    )\n",
    "  ) %>%\n",
    "  group_by(depdelay_groups) %>%\n",
    "  tally()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f16386",
   "metadata": {
    "lines_to_next_cell": 2,
    "name": "santoku04"
   },
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  filter(!is.na(DepDelay)) %>%\n",
    "  mutate(\n",
    "    depdelay_groups = chop_quantiles(\n",
    "      DepDelay, seq(0.1, 0.9, by = 0.1)\n",
    "    )\n",
    "  ) %>%\n",
    "  group_by(depdelay_groups) %>%\n",
    "  tally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1901b-1022-4913-889c-ac494dc11be2",
   "metadata": {},
   "source": [
    "You could also ask for groups such that they have the same width. Below we create 4 groups. Note that the width of each group is exactly 337.5 ... \n",
    "1323 - 985.5 = 337.5; 985.5 - 648 = 337.5; and so on. In this particular example this chopping isn't very useful since we end up with almost all of the data in the very first group. However, there are occasions where equal widths are useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1997e-8c18-446a-8d1b-630712d08b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmhflights %>%\n",
    "  filter(!is.na(DepDelay)) %>%\n",
    "  mutate(\n",
    "    depdelay_groups = chop_evenly(\n",
    "        DepDelay,\n",
    "        4\n",
    "    )\n",
    "  ) %>%\n",
    "  group_by(depdelay_groups) %>%\n",
    "  tally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d882dcd",
   "metadata": {},
   "source": [
    "### ordered()\n",
    "More often than we would like to see happen, we often end up with categorical variables that should follow a certain order but do not. For example, say you have survey data where people were asked to respond whether they Agree, are Neutral, or Disagree with some statement. Let us also assume that the frequencies are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a0d93-fa95-4f89-a49b-980bde07e498",
   "metadata": {
    "name": "ordered01"
   },
   "outputs": [],
   "source": [
    "tibble(\n",
    "  response = c(\n",
    "      rep(\"Agree\", 25), \n",
    "      rep(\"Neutral\", 30), \n",
    "      rep(\"Disagree\", 45)\n",
    "      )\n",
    "    ) -> mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e00fb-729f-4bae-ab7e-3250876e563b",
   "metadata": {
    "name": "ordered01"
   },
   "outputs": [],
   "source": [
    "mydf %>%\n",
    "  group_by(response) %>%\n",
    "  tally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb08c16",
   "metadata": {},
   "source": [
    "Notice how the responses are out of order, with Agree followed by Disagree, then Neutral, since R defaults to alphabetic ordering for anything that is a categorical variable. One way to ensure the correct ordering of categorical variables is via `ordered`, as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad0598",
   "metadata": {
    "lines_to_next_cell": 0,
    "name": "ordered02"
   },
   "outputs": [],
   "source": [
    "mydf %>%\n",
    "  mutate(\n",
    "      ordered.response = ordered(\n",
    "          response,\n",
    "          levels = c(\"Agree\", \"Neutral\", \"Disagree\")\n",
    "      )\n",
    "    ) %>%\n",
    "  group_by(ordered.response) %>%\n",
    "  tally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654b84d-688f-4b53-84d5-fed825db814e",
   "metadata": {},
   "source": [
    "## Concluding thoughts \n",
    "We have a covered a lot of ground here but every inch has been critical space. Google any question we have tackled and you will see how many R-users ask the same questions ... how do I calculate mean for groups in R? What you have seen is the heart of the `dplyr()` package. We saw grouped operations, we saw the use of summarise, mutate, case_when, distinct, filter, arrange, select, count, and tally. I will let you in on a secret; while these are core functions, there are others you could experiment with. Look up the cheat-sheet [here](https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf). \n",
    "\n",
    "Practice what we have done, with the `{nycflights13}` data-set perhaps to get something familiar yet sufficiently different to test your fundamentals. Maybe pick a non-travel data-set altogether, perhaps one of the `tidytuesday` data-sets. What is that you ask? Discover it for yourself [here](https://github.com/rfordatascience/tidytuesday). Bon voyage! Don't go too far because we will be working with two new packages next week -- `{tidyr}` and `{lubridate}`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a34876-0358-475f-b5bd-407223889d3f",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2616621a-02c2-470e-b48c-52d7fbf5ccca",
   "metadata": {},
   "source": [
    "# Exercises for Practice \n",
    "## Exercise 01\n",
    "Why are our best and most experienced employees leaving prematurely? \n",
    "\n",
    "The data [available here](https://aniruhil.github.io/avsr/teaching/dataviz/HR_comma_sep.csv) includes information on several current and former employees of an anonymous organization. Fields in the data-set include: \n",
    "\n",
    "| Variable | Description |\n",
    "| :---- | :---- |\n",
    "| satisfaction_level | = Level of satisfaction (numeric; 0-1) |\n",
    "| last_evaluation | = Evaluation score of the employee (numeric; 0-1) |\n",
    "| number_project | = Number of projects completed while at work (numeric) |\n",
    "| average_monthly_hours | = Average monthly hours spent at the workplace (numeric)  |\n",
    "| time_spend_company | = Number of years spent in the company (numeric) |\n",
    "| Work_accident | = Whether the employee had a workplace accident (categorical; 1 = yes or 0 = no) |\n",
    "| left | = Whether the employee left the workplace or not (categorical; 1 = left or 0 = stayed)  |\n",
    "| promotion_last_5years | = Whether the employee was promoted in the last five years (categorical; 1 = yes or 0 = no) |\n",
    "| sales | = Department in which they work (categorical) |\n",
    "| salary | = Relative level of salary (categorical; low, med, and high) |\n",
    "\n",
    "(a) Read in the csv-format data-set, naming it `hrdata` and save it in RData format as `hrdata.RData` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba40c0-2a0f-4b9c-aa61-e12bb0762019",
   "metadata": {},
   "source": [
    "(b) Create new variables that add labels to Work_accident, left, promotion_last_5years, and add these three to `hrdata`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36b0fc-107d-4512-9245-3fbc78f31a26",
   "metadata": {},
   "source": [
    "(c) Now retain only employees who left the company, and had not been promoted in the last five years. Save this result as `hr01`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3b2a8-22ca-4249-ac04-3a65e14cf742",
   "metadata": {},
   "source": [
    "(d) In this `hr01` data-set, how many employees do you have per sales department? What sales department has the most number of employees? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e4660-5dde-4f2d-8fc9-aeda1481794c",
   "metadata": {},
   "source": [
    "(e) By sales department, calculate mean and standard deviation of (i) satisfaction_level, and (ii) last_evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cf482-78b5-4d86-abe7-84e18a41aa6d",
   "metadata": {},
   "source": [
    "(f) What department has the lowest mean satisfaction? Which department has the highest variation in satisfaction?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5777b1d-311f-48dc-81bc-92e0e2a3bf80",
   "metadata": {},
   "source": [
    "(g) Create a new variable that groups `average_montly_hours` into 4 groups. You can let the group cut-points be chosen automatically with `chop_evenly()`. Then show the frequencies of each group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3890c308-5055-47d9-9fd6-7348b17b3e7d",
   "metadata": {},
   "source": [
    "## Exercise 02\n",
    "Thanks to the frenetic work of many individuals, the global spread of the Novel Coronavirus (COVID-19) has been tracked and the data made available for analysis. [Yanchang Zhao](https://rdatamining.wordpress.com/2020/03/10/coronavirus-data-analysis-with-r-tidyverse-and-ggplot2/) is one such individual and for this exercise we will use a spcific version of his data that I have named `cvdata.RData` is available in the `data` folder. Read it in via the `load()` command. We can then answer a few questions. Note the contents: \n",
    "\n",
    "+ `country =` name of the country \n",
    "+ `date =` date of incidents as recorded \n",
    "+ `confirmed =` cumulative count of the number of people who tested positive  \n",
    "+ `deaths =` cumulative count of the number of people lost to Covid-19 \n",
    "+ `deaths =` cumulative count of the number of people recovered  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a230cba-4e87-41f0-9e81-69e2da1f388b",
   "metadata": {},
   "source": [
    "(a) Filter the data-set so that we have only one row per country, the data from March 10, 2020 and call it `cv0310`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ab241-f744-4842-b584-8c77a263e124",
   "metadata": {},
   "source": [
    "(b) How many countries have lost `at least one` person to this tragedy? \"Others\" should not show up as one of the countries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198639e-5efe-4443-85c2-8528191e73d4",
   "metadata": {},
   "source": [
    "(c) What 10 countries have had the most number of confirmed cases? \"Others\" should not show up as one of the countries. Also ensure the result is organized in descending order of the number of confirmed cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed90a5fa-d6b4-4751-91d8-8da7d742ad4d",
   "metadata": {},
   "source": [
    "(d) Calculate the `fatality_rate`, defined for our purposes as the percent of deaths. excluding \"Others\", and only keeping countries that have had `at least 10` confirmed cases, arrange the result to show the top-10 countries in descending order of `fatality_rate`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e8938-83e4-4679-a602-c7b0430f9436",
   "metadata": {},
   "source": [
    "(e) Say we only want to focus on the Baltic countries (Estonia, Latvia, and Lithuania) as a unified group and compare this group to the ASEAN nations (Brunei, Cambodia, Indonesia, Laos, Malaysia, Myanmar, Philippines, Singapore, Thailand, and Vietnam). Use `cv0310` to complete the followng tasks: \n",
    "\n",
    "(i) Create a new variable called `region` that only takes on two values -- \"Baltic\" if the country is a Baltic country and \"Asean\" if the country is an ASEAN country. \n",
    "\n",
    "(ii) Use this variable to calculate the total number of confirmed cases in each region. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "include,tags,name,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
